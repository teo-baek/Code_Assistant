import os
import csv
import time
import warnings
import socket
from datetime import datetime
import streamlit as st

# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì„¸ì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")

try:
    from code_indexer import embed_project
    from rag_agent import LocalRAGAgent
except ImportError:
    st.error("í•„ìˆ˜ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# ì„¤ì •
BASE_DB_PATH = "./chroma_db"
EMBEDDING_MODEL_NAME = "BAAI/bge-small-en-v1.5"
DEFAULT_MODEL = "qwen2.5-coder:7b"
OLLAMA_BASE_URL = "http://localhost:11434"
FEEDBACK_FILE = "rag_feedback.csv"

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Agentic Code Assistant", layout="wide")
st.markdown(
    """
### Agentic Co-Developer: 
ì´ ë„êµ¬ëŠ” Agentic RAG ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬, ê²€ìƒ‰ëœ ì½”ë“œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ê²€ì¦í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
"""
)


# ë¡œì»¬ IP í™•ì¸ í•¨ìˆ˜
def get_local_ip():
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        return ip
    except:
        return "127.0.0.1"


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def get_existing_projects():
    """chroma_db í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ í•™ìŠµëœ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not os.path.exists(BASE_DB_PATH):
        return []
    # í´ë”ì´ë©´ì„œ ìˆ¨ê¹€ íŒŒì¼ì´ ì•„ë‹Œ ê²ƒë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ì—…
    return sorted(
        [
            d
            for d in os.listdir(BASE_DB_PATH)
            if os.path.isdir(os.path.join(BASE_DB_PATH, d)) and not d.startswith(".")
        ]
    )


# í•¨ìˆ˜: íŒŒì¼ íŠ¸ë¦¬ ìƒì„± (Context Map)
def generate_file_tree(startpath):
    """í”„ë¡œì íŠ¸ì˜ ì „ì²´ ì§€ë„ë¥¼ ê·¸ë ¤ì£¼ì–´, ê°œë°œìê°€ ì–´ë””ë¥¼ ìˆ˜ì •í•´ì•¼ í• ì§€ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ê²Œ ë•ìŠµë‹ˆë‹¤."""
    if not startpath or not os.path.exists(startpath):
        return "(ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)"
    startpath = os.path.abspath(startpath)
    tree_lines = []

    for root, dirs, files in os.walk(startpath):
        # ì •ë ¬í•˜ì—¬ ì¶œë ¥ ìˆœì„œ ê³ ì •
        dirs.sort()
        files.sort()
        dirs[:] = [d for d in dirs if not d.startswith(".")]  # ìˆ¨ê¹€ í´ë” ì œì™¸

        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°ìœ¼ë¡œ ì •í™•í•œ ê¹Šì´ íŒŒì•…
        rel_path = os.path.relpath(root, startpath)
        if rel_path == ".":
            level = 0
            # ë£¨íŠ¸ í´ë”ëª… ì¶œë ¥
            tree_lines.append(f"{os.path.basename(startpath)}/")
        else:
            level = rel_path.count(os.sep) + 1
            indent = "    " * (level - 1)
            folder_name = os.path.basename(root)
            # í•˜ìœ„ í´ë”ëª… ì¶œë ¥ (ë“¤ì—¬ì“°ê¸° ì ìš©)
            tree_lines.append(f"{indent} {folder_name}/")

        # íŒŒì¼ ì¶œë ¥
        sub_indent = "    " * level
        for f in files:
            if not f.startswith("."):
                tree_lines.append(f"{sub_indent} {f}")

    return "\n".join(tree_lines) if tree_lines else "(ë¹ˆ í´ë”ì…ë‹ˆë‹¤.)"


# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("í™˜ê²½ ì„¤ì •")

    # 1. ëª¨ë¸ ì„ íƒ
    model_options = ["qwen2.5-coder:7b", "llama3", "codellama", "mistral"]
    selected_model = st.selectbox(
        "LLM ëª¨ë¸ ì„ íƒ",
        model_options,
        index=0,
        help="Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”. (ê¸°ë³¸ê°’: qwen2.5-coder)",
    )

    st.divider()

    # 2. í”„ë¡œì íŠ¸ ì„ íƒ
    st.subheader("í”„ë¡œì íŠ¸ ê´€ë¦¬")
    existing_projects = get_existing_projects()

    tab1, tab2 = st.tabs(["ë¶ˆëŸ¬ì˜¤ê¸°", "ìƒˆë¡œ í•™ìŠµ"])

    project_name = None

    with tab1:
        if existing_projects:
            project_name = st.selectbox("í•™ìŠµëœ í”„ë¡œì íŠ¸", existing_projects)
            st.success(f"'{project_name}' ì¤€ë¹„ë¨")
        else:
            st.info("í•™ìŠµëœ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab2:
        new_project_name = st.text_input(
            "ìƒˆ í”„ë¡œì íŠ¸ ì´ë¦„ (DBëª…)", placeholder="my-project"
        )
        new_root_path = st.text_input("ì‹¤ì œ íŒŒì¼ ê²½ë¡œ", placeholder="C:/Work/MyProject")

        if st.button("DB í•™ìŠµ ì‹œì‘", type="primary"):
            if not new_project_name or not new_root_path:
                st.error("ì´ë¦„ê³¼ ê²½ë¡œë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                with st.spinner(f"'{new_project_name}' í•™ìŠµ ì¤‘"):
                    success, msg = embed_project(new_root_path, new_project_name)
                    if success:
                        st.success(msg)
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(msg)
        if new_project_name and not project_name:
            project_name = new_project_name

    st.divider()
    project_root_path = st.text_input(
        "íŒŒì¼ íŠ¸ë¦¬ ê²½ë¡œ(ì„ íƒ)", help="íŒŒì¼ êµ¬ì¡° ì‹œê°í™”ë¥¼ ìœ„í•œ ì‹¤ì œ ê²½ë¡œ"
    )

    if project_root_path and os.path.isdir(project_root_path):
        with st.expander("íŒŒì¼ êµ¬ì¡°"):
            st.code(generate_file_tree(project_root_path), language="text")

    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°"):
        st.session_state.messages = []
        st.rerun()

    user_id = st.text_input("ê°œë°œì ID", value="Dev_User")


# RAG ì—ì´ì „íŠ¸ ë¡œë“œ
@st.cache_resource
def load_agent(prj_name, model_name):
    """í”„ë¡œì íŠ¸ DBì™€ ì„ íƒëœ LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    db_path = os.path.join(BASE_DB_PATH, prj_name)
    if not os.path.exists(db_path):
        return None

    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    try:
        vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

        # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ê·¸ë˜í”„ ë¹Œë“œ
        agent_instance = LocalRAGAgent(retriever, model_name, OLLAMA_BASE_URL)
        app_graph = agent_instance.build_graph()

        return app_graph
    except Exception as e:
        return str(e)


# í”¼ë“œë°± ë¡œê¹… í•¨ìˆ˜
def log_feedback(project, user, question, answer, rating, docs):
    file_exists = os.path.isfile(FEEDBACK_FILE)
    with open(FEEDBACK_FILE, mode="a", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(
                [
                    "Time",
                    "Project",
                    "User",
                    "Question",
                    "Answer",
                    "Rating",
                    "Context_Files",
                ]
            )

        # ë¬¸ì„œ ê°ì²´ì—ì„œ ì†ŒìŠ¤ë§Œ ì¶”ì¶œ
        sources = [d.metadata.get("source", "Unknown") for d in docs] if docs else []
        writer.writerow(
            [datetime.now(), project, user, question, answer, rating, str(sources)]
        )


# ë©”ì¸ ì‹¤í–‰ ë¡œì§
app_graph = None
current_tree = ""
system_msg = ""
is_ready = False

if project_name:
    # ì„ íƒëœ ëª¨ë¸ì„ ì¸ìë¡œ ì „ë‹¬
    result = load_agent(project_name, selected_model)

    # LangGraph ì»´íŒŒì¼ëœ ê°ì²´ì¸ì§€ í™•ì¸ (Callable í•˜ê±°ë‚˜ invoke ë©”ì„œë“œê°€ ìˆì–´ì•¼ í•¨)
    if result and hasattr(result, "invoke"):
        app_graph = result
        is_ready = True
        if project_root_path:
            current_tree = generate_file_tree(project_root_path)
    else:
        system_msg = "í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."

# ì±„íŒ… UI

if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message("role"):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if not app_graph:
            st.error("AIê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            try:
                with st.spinner(f"{selected_model}ê°€ ìƒê°í•˜ê³  ê²€ì¦í•˜ëŠ” ì¤‘"):
                    # LangGraph ì‹¤í–‰
                    inputs = {
                        "question": prompt,
                        "project_name": project_name,
                        "file_tree": current_tree,
                    }

                    final_state = app_graph.invoke(inputs)
                    answer = final_state.get(
                        "generation", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    )

                    st.markdown(answer)

                    # ê²€ì¦ëœ ë¬¸ì„œë§Œ ê·¼ê±°ë¡œ í‘œì‹œ
                    valid_docs = final_state.get("documents", [])
                    if valid_docs:
                        with st.expander(f"ê²€ì¦ëœ ê·¼ê±° ë¬¸ì„œ ({len(valid_docs)}ê°œ)"):
                            for doc in valid_docs:
                                st.caption(f"{doc.metadata.get('source', 'Unknown')}")
                                st.code(doc.page_content)
                    else:
                        st.caption(
                            "ê²€ìƒ‰ëœ ë¬¸ì„œ ì¤‘ ê´€ë ¨ì„± ë†’ì€ ì½”ë“œê°€ ì—†ì–´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤."
                        )
                st.session_state.messages.append(
                    {"role": "assistant", "content": answer}
                )

                # í”¼ë“œë°± ìƒíƒœ ì €ì¥
                st.session_state.last_interaction = {
                    "p": project_name,
                    "q": prompt,
                    "a": answer,
                    "d": valid_docs,
                }
                st.rerun()

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")

# í”¼ë“œë°± UI
if (
    is_ready
    and "last_interaction" in st.session_state
    and st.session_state.last_interaction
):
    st.divider()
    cols = st.columns([1, 1, 6])
    last = st.session_state.last_interaction

    if cols[0].button("ğŸ‘"):
        log_feedback(last["p"], user_id, last["q"], last["a"], "Good", last["d"])
        st.toast("í”¼ë“œë°± ì €ì¥ë¨!")
        del st.session_state.last_interaction
        st.rerun()

    if cols[1].button("ğŸ‘"):
        log_feedback(last["p"], user_id, last["q"], last["a"], "Bad", last["d"])
        st.toast("í”¼ë“œë°± ì €ì¥ë¨!")
        del st.session_state.last_interaction
        st.rerun()
